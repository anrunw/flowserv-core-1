# This file is part of the Reproducible and Reusable Data Analysis Workflow
# Server (flowServ).
#
# Copyright (C) [2019-2020] NYU.
#
# flowServ is free software; you can redistribute it and/or modify it under the
# terms of the MIT License; see LICENSE file for more details.

"""Unit test for downloading results of a post-processing workflow."""

import os
import pytest
import time

from flowserv.controller.serial.engine import SerialWorkflowEngine
from flowserv.model.workflow.resource import FSObject
from flowserv.service.postproc.client import Runs
from flowserv.tests.files import FakeStream

import flowserv.core.error as err
import flowserv.core.util as util
import flowserv.model.workflow.state as st
import flowserv.service.postproc.util as postproc
import flowserv.tests.service as service


DIR = os.path.dirname(os.path.realpath(__file__))
TEMPLATE_DIR = os.path.join(DIR, '../.files/benchmark/helloworld')
SPEC_FILE = os.path.join(DIR, '../.files/benchmark/postproc/benchmark.yaml')


def test_workflow_postproc_results(tmpdir):
    """Test downloading results from a post-processing workflow."""
    # Initialize the database and the API. Create one workflow with four groups
    api, engine, workflows = service.init_service(
        basedir=str(tmpdir),
        templatedir=TEMPLATE_DIR,
        specfile=SPEC_FILE,
        engine=SerialWorkflowEngine(is_async=False),
        wf_count=1,
        gr_count=4
    )
    w_id, groups = workflows[0]
    # Before the first run the workflow does not have a post-processing run
    # associated with it. Accessing the post-processing resources will raise
    # an error.
    with pytest.raises(err.UnknownResourceError):
        api.workflows().get_result_archive(workflow_id=w_id)
    with pytest.raises(err.UnknownResourceError):
        api.workflows().get_result_file(workflow_id=w_id, resource_id='0')
    # Start a single run for each of the workflow groups.
    run_count = 0
    names = list()
    for g_id, u_id in groups:
        # Create a the names file that is used as input for the benchmark run.
        # Here we use the list of group names as input.
        names.append(g_id)
        r = api.uploads().upload_file(
            group_id=g_id,
            file=FakeStream(data=names, format='txt/plain'),
            name='names.txt',
            user_id=u_id
        )
        file_id = r['id']
        r = api.runs().start_run(
            group_id=g_id,
            arguments=[{'id': 'names', 'value': file_id}],
            user_id=u_id
        )
        # The run should be successfull
        assert r['state'] == st.STATE_SUCCESS
        run_count += 1
        # Get the workflow handle. At this point, the post-processing run may
        # not have started. Wait until it starts. Add 'watch-dog' counter to
        # avoid endless-loop in case of an error
        w = api.workflows().get_workflow(w_id)
        counter = 0
        while 'postproc' not in w:
            # The post-processing workflow should start within 10 seconds.
            counter += 1
            if counter == 10:
                break
            time.sleep(1)
            w = api.workflows().get_workflow(w_id)
        assert counter < 10
        api.workflows().get_result_archive(workflow_id=w_id)
        # Get the file handle for the comparison result file. This assumes that
        # there is only one resource generated by the post-processing workflow.
        fh = api.workflows().get_result_file(
            workflow_id=w_id,
            resource_id=w['postproc']['resources'][0]['id']
        )
        results = util.read_object(fh.filename)
        assert len(results) == run_count
        total_counts = sorted([doc['total_count'] for doc in results])
        assert len(total_counts) == run_count
        for i in range(len(total_counts)):
            assert total_counts[i] == (i + 1) * total_counts[0]
